Top test accuracies for different params. Modifications were performed in the order they are listed below.

Initial params:
{
    'max_epochs': 70,
    'lr': 3e-07,
    'batch_size': 16,

    'optimizer': 'AdamW',
    'optimizer__weight_decay': 0.05,

    'callbacks__default_lr_scheduler__policy': 'CosineAnnealingLR',
    'callbacks__default_lr_scheduler__T_max': 50,
    'callbacks__default_lr_scheduler__eta_min': 1e-07,

    'module__vit_model_variant': 'vit_l_16',
    'module__pretrained': True,
    'module__unfreeze_strategy': 'encoder_tail',
    'module__num_transformer_blocks_to_unfreeze': 2,
    'module__unfreeze_cls_token': True,
    'module__unfreeze_pos_embedding': True,
    'module__unfreeze_patch_embedding': False,
    'module__unfreeze_encoder_layernorm': True,
    'module__custom_head_hidden_dims': None,
    'module__head_dropout_rate': 0.55
}

module__vit_model_variant: vit_l_16 (0.1597)
module__vit_model_variant: vit_l_16 -> vit_b_16 (0.2948)

lr: 3e-07 -> 3e-2 (0.3587) very random and risky
lr: -> 3e-4 (0.5061) overfitting from epoch 2

callbacks__default_lr_scheduler__T_max: from 50 to 70 (0.5160)

lr: 3e-4 -> 3e-5 (0.4963) -> 1e-3 (0.5209) -> 3e-6 (0.4668) -> 1e-4

optimizer__weight_decay: 0.05 -> 0.35 (0.5061) -> 0.85 (0.4988) -> 0.05 (0.5012) -> 0.35

lr: 1e-4 -> 1e-3 (0.5307)

module__head_dropout_rate: 0.55 -> 0.85 (0.5184) -> 0.35 (0.5086) -> 0.65 (0.5111) -> 0.45 (0.5160)

callbacks__default_lr_scheduler__eta_min: 1e-07 -> 1e-03 (0.5307)

module__num_transformer_blocks_to_unfreeze: 2 -> 4 (0.5012) -> 1 (0.5160) -> 2

module__custom_head_hidden_dims: None -> [256] (0.5233) -> [512] (0.5184)

--- Fom here on the validation loss will be considered as the main metric ---

module__custom_head_hidden_dims: [512] (1.4564) -> [512, 256] (1.5797)

params with best valid loss (1.4357)
{
    'max_epochs': 70,
    'lr': 0.001,
    'batch_size': 16,

    'optimizer': 'AdamW',
    'optimizer__weight_decay': 0.05,

    'callbacks__default_lr_scheduler__policy': 'CosineAnnealingLR',
    'callbacks__default_lr_scheduler__T_max': 70,
    'callbacks__default_lr_scheduler__eta_min': 1e-07,

    'module__vit_model_variant': 'vit_b_16',
    'module__pretrained': True,
    'module__unfreeze_strategy': 'encoder_tail',
    'module__num_transformer_blocks_to_unfreeze': 2,
    'module__unfreeze_cls_token': True,
    'module__unfreeze_pos_embedding': True,
    'module__unfreeze_patch_embedding': False,
    'module__unfreeze_encoder_layernorm': True,
    'module__custom_head_hidden_dims': None,
    'module__head_dropout_rate': 0.55
}

optimizer__weight_decay: 0.05 (1.4357) | 0.10 (1.5008) | 0.15 (1.4940) | 0.25 (1.4728) | 0.45 (1.4473) | 0.55 (1.4755) | 0.85 (1.4991)
kept 0.05

lr: 1e-3 (1.4357) | 2e-4 (1.4924) | 5e-5 (1.4980) | 1e-5 (1.4882, Test acc: 0.5226) | 2e-6 (1.5762, reached 70 epochs)
kept 2e-6

using these params (new lr scheduler)

pretrained_vit_fixed_params = {
    'max_epochs': 70,
    'lr': 2e-6,
    'batch_size': 16,

    'optimizer': 'AdamW',
    'optimizer__weight_decay': 0.05, # Start with original, can reduce later

    'callbacks__default_lr_scheduler__policy': 'CosineAnnealingWarmRestarts',
    'callbacks__default_lr_scheduler__T_0': 15,  # Epochs for the first cycle
    'callbacks__default_lr_scheduler__T_mult': 1,  # Subsequent cycles are same length as T_0

    'callbacks__default_lr_scheduler__eta_min': 1e-07,

    'callbacks__default_early_stopping__patience': 20,

    'module__vit_model_variant': 'vit_b_16',
    'module__pretrained': True,
    'module__unfreeze_strategy': 'encoder_tail',
    'module__num_transformer_blocks_to_unfreeze': 2,
    'module__unfreeze_cls_token': True,
    'module__unfreeze_pos_embedding': True,
    'module__unfreeze_patch_embedding': False,
    'module__unfreeze_encoder_layernorm': True,
    'module__custom_head_hidden_dims': None,
    'module__head_dropout_rate': 0.55
}

(1.5632, reached 70 epochs)

callbacks__default_lr_scheduler__eta_min: 1e-07 -> 8e-07 (1.5254)

changed back to previous params (1.4357, test acc: 0.4774)

optimizer__weight_decay: 0.05 -> 0.45 (1.4467, test acc: 0.4892)

callbacks__default_lr_scheduler__eta_min: 1e-07 -> 1e-05 (1.4539, test acc: 0.4912)

Augmentation modifications. Current:

    geometric_transforms = [
        transforms.Resize(img_size),  # Or a slightly larger size then RandomCrop
        transforms.RandomCrop(img_size),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=5), # Slightly increased rotation if tolerable
    ]

    color_intensity_transforms = [
        # transforms.ColorJitter(
        #     brightness=0.5,  # e.g., range [0.5, 1.5] of original brightness
        #     contrast=0.5,    # e.g., range [0.5, 1.5] of original contrast
        #     saturation=0.4,  # e.g., range [0.6, 1.4] of original saturation
        #     hue=0.1          # e.g., range [-0.1, 0.1] for hue shift (max 0.5)
        # ),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.02),

        # Randomly apply auto-contrast to maximize image contrast
        # transforms.RandomAutocontrast(p=0.3),

        # Randomly equalize the image histogram - can significantly alter appearance
        # transforms.RandomEqualize(p=0.2),

        # Randomly convert to grayscale sometimes
        # transforms.RandomGrayscale(p=0.1),
    ]

    blur_transform = [
        # transforms.RandomApply([
        #     transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)) # Slightly increased max sigma
        # ], p=0.3), # Increased probability of blur
        transforms.RandomApply([
            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))
        ], p=0.2),
    ]

    # Final conversions
    final_transforms = [
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]

    return transforms.Compose(
        geometric_transforms +
        color_intensity_transforms +
        blur_transform +
        final_transforms
    )

RandomRotation.degrees: 0 (v loss: 1.6086, ts acc: 0.4440) | 5 | 10 (v loss: 1.4778, ts acc:0.4715 ) | 15 (v loss: 1.5070, ts acc: 0.4872) | 45 (v loss: 1.4486, ts acc: 0.4892) | 90 (v loss: 1.4873, ts acc: 0.4617) | 180 (v loss: 1.5016, ts acc: 0.4990)
kept 5

use more aggressive color jittering: (v loss: 1.4830, ts acc: 0.4892)
more aggressive color jittering: (v loss: 1.4871, ts acc: 0.4951)
moderate color jittering: (v loss: 1.4638, ts acc: 0.5029)
more moderate color jittering (less aggressive than initial): (v loss: 1.4874, ts acc: 0.4735)
no color jittering: (v loss: 1.4874, ts acc: 0.5029)
kept initial color jittering

added random autocontrast: (v loss: 1.4707, ts acc: 0.5128)
removed random autocontrast

added random equalize: (v loss: 1.5214, ts acc: 0.4715)
removed random equalize

added random grayscale: (v loss: 1.4884, ts acc: 0.5167)
removed random grayscale

use enhanced random blur: (v loss: 1.4603, ts acc: 0.5029)
