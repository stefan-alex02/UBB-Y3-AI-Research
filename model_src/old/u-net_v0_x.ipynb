{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-22T00:29:17.742622Z",
     "start_time": "2024-11-22T00:29:14.948204Z"
    }
   },
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.models.resnet import ResnetBlock2D\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define Dataset and Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "data_dir = \"../modified-mini-GCD\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "train_dataset = ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Class Names\n",
    "class_names = train_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# 2. Load Stable Diffusion and Access U-Net\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "unet = pipeline.unet  # Access the U-Net\n",
    "unet.eval()  # Set to evaluation mode\n",
    "\n",
    "def adjust_groupnorm(unet):\n",
    "    for module in unet.modules():\n",
    "        if isinstance(module, ResnetBlock2D):\n",
    "            for name, sub_module in module.named_children():\n",
    "                if isinstance(sub_module, nn.GroupNorm):\n",
    "                    num_channels = sub_module.num_channels\n",
    "                    # Determine a valid number of groups\n",
    "                    num_groups = min(32, num_channels)  # Use a reasonable maximum (e.g., 32 groups)\n",
    "                    while num_channels % num_groups != 0:\n",
    "                        num_groups -= 1  # Reduce until divisible\n",
    "                    if num_groups <= 0:\n",
    "                        num_groups = 1  # Fallback to 1 group if no valid groups found\n",
    "\n",
    "                    # Replace the GroupNorm layer\n",
    "                    new_groupnorm = nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)\n",
    "                    print(f\"Replacing GroupNorm with {num_groups} groups in {name}\")\n",
    "                    setattr(module, name, new_groupnorm)\n",
    "    return unet\n",
    "\n",
    "# Apply this adjustment to your U-Net\n",
    "unet = adjust_groupnorm(unet)\n",
    "\n",
    "# 3. Modify U-Net to Extract Encoder Features\n",
    "class UNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, unet, in_channels=3):\n",
    "        super(UNetFeatureExtractor, self).__init__()\n",
    "        # Input mapping layer to match U-Net's expected in_channels\n",
    "        self.input_conv = nn.Conv2d(in_channels, unet.config.in_channels, kernel_size=3, padding=1)\n",
    "        self.encoder = nn.ModuleList(unet.down_blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_conv(x)  # Match input channels to U-Net's expected channels\n",
    "        features = []\n",
    "        for block in self.encoder:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "        return features[-1]\n",
    "\n",
    "# Instantiate the feature extractor\n",
    "feature_extractor = UNetFeatureExtractor(unet, in_channels=3)  # Use 3 channels for RGB input\n",
    "\n",
    "# 4. Build Classifier Using Extracted Features\n",
    "class DiffusionClassifier(nn.Module):\n",
    "    def __init__(self, feature_extractor, num_classes):\n",
    "        super(DiffusionClassifier, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        # Assume the output feature size is large (adjust based on U-Net output)\n",
    "        self.fc = nn.Linear(1280, num_classes)  # Adjust input size accordingly\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():  # Freeze feature extractor\n",
    "            features = self.feature_extractor(x)\n",
    "            features = features.mean(dim=(2, 3))  # Global Average Pooling\n",
    "        out = self.fc(features)\n",
    "        return out\n",
    "\n",
    "# Instantiate classifier\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(class_names)\n",
    "classifier = DiffusionClassifier(feature_extractor, num_classes).to(device)\n",
    "\n",
    "# 5. Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.fc.parameters(), lr=1e-4)\n",
    "\n",
    "def preprocess_input(inputs):\n",
    "    # Ensure the input has 3 channels (RGB)\n",
    "    if inputs.shape[1] != 3:  # Check if input is not 3 channels (RGB)\n",
    "        raise ValueError(\"Input must have 3 channels (RGB).\")\n",
    "    return inputs\n",
    "\n",
    "# 6. Training Loop\n",
    "def train_model(classifier, dataloader, epochs=5):\n",
    "    classifier.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = preprocess_input(inputs).to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = classifier(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(classifier, train_loader, epochs=5)\n",
    "\n",
    "# Save the model\n",
    "torch.save(classifier.state_dict(), \"diffusion_classifier.pth\")\n",
    "\n",
    "# Load the model\n",
    "classifier.load_state_dict(torch.load(\"diffusion_classifier.pth\"))\n",
    "\n",
    "# 7. Evaluate the Model\n",
    "def evaluate_model(classifier, dataloader):\n",
    "    classifier.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = classifier(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "preds, labels = evaluate_model(classifier, test_loader)\n",
    "\n",
    "# 8. Calculate Accuracy\n",
    "accuracy = np.mean(np.array(preds) == np.array(labels))\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['1_clearsky', '2_cloudy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ebd4ec78997a4d649aecf35f97e6e2a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n",
      "Replacing GroupNorm with 32 groups in norm1\n",
      "Replacing GroupNorm with 32 groups in norm2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'UNetFeatureExtractor' object has no attribute 'unet'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 124\u001B[0m\n\u001B[0;32m    121\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrunning_loss\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(dataloader)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m--> 124\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;66;03m# Save the model\u001B[39;00m\n\u001B[0;32m    127\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(classifier\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiffusion_classifier.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[3], line 116\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(classifier, dataloader, epochs)\u001B[0m\n\u001B[0;32m    114\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m preprocess_input(inputs)\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    115\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 116\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mclassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    117\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m    118\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\UBB-Y3-AI-Research\\DiffusionClassifier_initial\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\UBB-Y3-AI-Research\\DiffusionClassifier_initial\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[3], line 88\u001B[0m, in \u001B[0;36mDiffusionClassifier.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():  \u001B[38;5;66;03m# Freeze feature extractor\u001B[39;00m\n\u001B[1;32m---> 88\u001B[0m         features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     89\u001B[0m         features \u001B[38;5;241m=\u001B[39m features\u001B[38;5;241m.\u001B[39mmean(dim\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m))  \u001B[38;5;66;03m# Global Average Pooling\u001B[39;00m\n\u001B[0;32m     90\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(features)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\UBB-Y3-AI-Research\\DiffusionClassifier_initial\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\UBB-Y3-AI-Research\\DiffusionClassifier_initial\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[3], line 73\u001B[0m, in \u001B[0;36mUNetFeatureExtractor.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     71\u001B[0m timesteps \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\u001B[38;5;241m.\u001B[39mto(x\u001B[38;5;241m.\u001B[39mdevice)  \u001B[38;5;66;03m# Dummy timesteps\u001B[39;00m\n\u001B[0;32m     72\u001B[0m encoder_hidden_states \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros_like(x)  \u001B[38;5;66;03m# Dummy encoder states\u001B[39;00m\n\u001B[1;32m---> 73\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munet\u001B[49m(x, timesteps, encoder_hidden_states)\u001B[38;5;241m.\u001B[39msample\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\UBB-Y3-AI-Research\\DiffusionClassifier_initial\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1929\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[0;32m   1930\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[1;32m-> 1931\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[0;32m   1932\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1933\u001B[0m )\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'UNetFeatureExtractor' object has no attribute 'unet'"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
