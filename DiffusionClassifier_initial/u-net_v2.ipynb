{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# U-Net Fine-Tuning for Image Classification on Modified Mini-GCD Dataset with Classification Head",
   "id": "d508bcd9025f91fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 0. Setup Environment",
   "id": "de422169c9240b10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 0.1. Install Required Libraries",
   "id": "139c5354d63026af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T00:54:31.038598Z",
     "start_time": "2024-11-22T00:54:31.033402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "861158d1a7e9a24d",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 0.2. Check GPU Availability",
   "id": "ce944d40d57c4575"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T00:54:31.146265Z",
     "start_time": "2024-11-22T00:54:31.140989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "8b6058ed2318b39d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce GTX 1080 Ti\n",
      "cuda\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T00:54:31.155853Z",
     "start_time": "2024-11-22T00:54:31.152272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 50"
   ],
   "id": "efeeeaed9d2d7dca",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Define Dataset and Transformations",
   "id": "cb6d59f66993299d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T00:54:31.305582Z",
     "start_time": "2024-11-22T00:54:31.297135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "data_dir = \"modified-mini-GCD\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir, \"test\")  # Assuming you have a validation set\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "train_dataset = ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = ImageFolder(root=val_dir, transform=transform)\n",
    "test_dataset = ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ],
   "id": "11bed79af35758a9",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Load the pre-trained U-Net model",
   "id": "dbe26ecaa3e30e49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2.1. Load the Pre-trained U-Net Model and Print the Number of Trainable Parameters",
   "id": "919cb30ffad27c1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T00:54:31.953835Z",
     "start_time": "2024-11-22T00:54:31.311043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Load the pre-trained U-Net model\n",
    "unet = smp.Unet(encoder_name=\"resnet34\", encoder_weights=\"imagenet\", classes=1, activation=None)\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Number of trainable parameters in U-Net: {count_parameters(unet)}')"
   ],
   "id": "4aa6bc6c5195e3e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in U-Net: 24436369\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2.2. Add a Classification Head to the U-Net",
   "id": "58c0b1d28d4514aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T00:54:32.035079Z",
     "start_time": "2024-11-22T00:54:32.027994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UNetClassifier(nn.Module):\n",
    "    def __init__(self, unet, num_classes):\n",
    "        super(UNetClassifier, self).__init__()\n",
    "        self.unet = unet\n",
    "        # Get the number of output channels from the final layer of the U-Net\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(unet.segmentation_head[0].out_channels, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.unet(x)\n",
    "        out = self.classifier(features)\n",
    "        return out\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "model = UNetClassifier(unet, num_classes)"
   ],
   "id": "2b9ae329802c4d50",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Prepare for Training",
   "id": "5d7e6fa9188a5c73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.1. Define Loss Function and Optimizer",
   "id": "3704bcceded5911e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T00:54:32.122415Z",
     "start_time": "2024-11-22T00:54:32.117181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "6e610909a002b848",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.2. Train the Classifier",
   "id": "5ae0ef1c7f489fad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T00:54:35.996534Z",
     "start_time": "2024-11-22T00:54:32.195699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training and validation loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
   ],
   "id": "c6b5caff15b05732",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.2645\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 15\u001B[0m\n\u001B[0;32m     12\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     13\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 15\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m inputs\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     17\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m running_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_loader\u001B[38;5;241m.\u001B[39mdataset)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.3. Plot Training and Validation Losses",
   "id": "7cb72b0a10b113cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f758f97403f1b566"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Persist the Model",
   "id": "113e05c4f32cc8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "\n",
    "# Save the trained model\n",
    "save_model(model, 'unet_classifier.pth')\n",
    "\n",
    "# Load the model back\n",
    "loaded_model = UNetClassifier(unet, num_classes)\n",
    "load_model(loaded_model, 'unet_classifier.pth')"
   ],
   "id": "73548feac3fb9ec2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Evaluate the model",
   "id": "848c5cbc58f6d55e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluate the model and print metrics\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f'Accuracy: {accuracy_score(all_labels, all_preds):.4f}')\n",
    "    print(f'Precision: {precision_score(all_labels, all_preds, average=\"weighted\"):.4f}')\n",
    "    print(f'Recall: {recall_score(all_labels, all_preds, average=\"weighted\"):.4f}')\n",
    "    print(f'F1 Score: {f1_score(all_labels, all_preds, average=\"weighted\"):.4f}')\n",
    "    print(f'AUC-ROC: {roc_auc_score(all_labels, all_probs, multi_class=\"ovr\"):.4f}')\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate the loaded model\n",
    "evaluate_model(loaded_model, test_loader, device)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
